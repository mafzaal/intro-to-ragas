{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbebd1d1",
   "metadata": {},
   "source": [
    "# Advanced Metrics and Customization\n",
    "\n",
    "Please read the full article at [thedataguy.pro](https://thedataguy.pro/blog/generating-test-data-with-ragas/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c55145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd8a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Set\n",
    "import typing as t\n",
    "\n",
    "from ragas.metrics.base import MetricWithLLM, SingleTurnMetric\n",
    "from ragas.prompt import PydanticPrompt\n",
    "from ragas.metrics import MetricType, MetricOutputType\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Define input/output models for the prompt\n",
    "class TechnicalAccuracyInput(BaseModel):\n",
    "    question: str\n",
    "    context: str\n",
    "    response: str\n",
    "    programming_language: str = \"python\"\n",
    "\n",
    "class TechnicalAccuracyOutput(BaseModel):\n",
    "    score: float\n",
    "    feedback: str\n",
    "\n",
    "\n",
    "# Define the prompt\n",
    "class TechnicalAccuracyPrompt(PydanticPrompt[TechnicalAccuracyInput, TechnicalAccuracyOutput]):\n",
    "    instruction: str = (\n",
    "        \"Evaluate the technical accuracy of the response to a programming question. \"\n",
    "        \"Consider syntax correctness, algorithmic accuracy, and best practices.\"\n",
    "    )\n",
    "    input_model = TechnicalAccuracyInput\n",
    "    output_model = TechnicalAccuracyOutput\n",
    "    examples = [\n",
    "        # Add examples here\n",
    "    ]\n",
    "\n",
    "# Create the metric\n",
    "@dataclass\n",
    "class TechnicalAccuracy(MetricWithLLM, SingleTurnMetric):\n",
    "    name: str = \"technical_accuracy\"\n",
    "    _required_columns: Dict[MetricType, Set[str]] = field(\n",
    "        default_factory=lambda: {\n",
    "            MetricType.SINGLE_TURN: {\n",
    "                \"user_input\",\n",
    "                \"response\",\n",
    "                \n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    output_type: Optional[MetricOutputType] = MetricOutputType.CONTINUOUS\n",
    "    evaluation_prompt: PydanticPrompt = field(default_factory=TechnicalAccuracyPrompt)\n",
    "    \n",
    "    async def _single_turn_ascore(self, sample, callbacks) -> float:\n",
    "        assert self.llm is not None, \"LLM must be set\"\n",
    "        \n",
    "        question = sample.user_input\n",
    "        response = sample.response\n",
    "        # Extract programming language from question if possible\n",
    "        programming_language = \"python\"  # Default\n",
    "        languages = [\"python\", \"javascript\", \"java\", \"c++\", \"rust\", \"go\"]\n",
    "        for lang in languages:\n",
    "            if lang in question.lower():\n",
    "                programming_language = lang\n",
    "                break\n",
    "        \n",
    "        # Get the context\n",
    "        context = \"\\n\".join(sample.retrieved_contexts) if sample.retrieved_contexts else \"\"\n",
    "        \n",
    "        # Prepare input for prompt\n",
    "        prompt_input = TechnicalAccuracyInput(\n",
    "            question=question,\n",
    "            context=context,\n",
    "            response=response,\n",
    "            programming_language=programming_language\n",
    "        )\n",
    "        \n",
    "        # Generate evaluation\n",
    "        evaluation = await self.evaluation_prompt.generate(\n",
    "            data=prompt_input, llm=self.llm, callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        return evaluation.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67f8330f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical Accuracy Score: 9.0\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from ragas import SingleTurnSample\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "# Initialize the LLM, you are going to OPENAI API key\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\")) \n",
    "\n",
    "test_data = {\n",
    "    \"user_input\": \"Write a function to calculate the factorial of a number in Python.\",\n",
    "    \"retrieved_contexts\": [\"Python is a programming language.\", \"A factorial of a number n is the product of all positive integers less than or equal to n.\"],\n",
    "    \"response\": \"def factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\",\n",
    "}\n",
    "\n",
    "# Create a sample\n",
    "sample = SingleTurnSample(**test_data)  # Unpack the dictionary into the constructor\n",
    "technical_accuracy = TechnicalAccuracy(llm=evaluator_llm)\n",
    "score = await technical_accuracy.single_turn_ascore(sample)\n",
    "print(f\"Technical Accuracy Score: {score}\")\n",
    "# Note: The above code is a simplified example. In a real-world scenario, you would need to handle exceptions,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
