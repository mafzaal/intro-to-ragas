{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval Evaulation\n",
        "\n",
        "Please read the full article at [thedataguy.pro](https://thedataguy.pro/blog/evaluating-advanced-rag-retrievers/).\n",
        "\n",
        "\n",
        "## Solution Approach\n",
        "\n",
        "### Step 1: Generate Synthetic Data\n",
        "\n",
        "We'll generate test data using RAGAS Synthetic Data Generator to create a comprehensive evaluation set for our retrievers. The synthetic data will be stored in CSV format for reuse across different retriever evaluations.\n",
        "\n",
        "> Note: `testset.csv` was generated using `grok-3` and `Snowflake/snowflake-arctic-embed-l` in the `grok_3.ipynb` notebook.\n",
        "\n",
        "### Step 2: Generate Eval Dataset\n",
        "\n",
        "We'll create a LangSmith dataset and run evaluation using the following RAGAS metrics:\n",
        "- `LLMContextRecall`: Measures how much of the relevant context the LLM incorporated\n",
        "- `Faithfulness`: Evaluates if the answers are grounded in the retrieved context\n",
        "- `ContextRecall`: Assesses how well the retriever found relevant documents\n",
        "- `AnswerRelevancy`: Measures how relevant the generated answers are to the questions\n",
        "\n",
        "### Step 3: Evaluation\n",
        "\n",
        "Each retriever chain will be evaluated using LangSmith experiments:\n",
        "- Naive Retriever (Vector similarity)\n",
        "- BM25 Retriever (Sparse retrieval)\n",
        "- Contextual Compression Retriever with Cohere Rerank\n",
        "- Multi-Query Retriever\n",
        "- Parent Document Retriever\n",
        "- Ensemble Retriever\n",
        "\n",
        "## Step 4: Analysis and Report\n",
        "\n",
        "We'll build comparison charts and tables across three key dimensions:\n",
        "- Performance: Based on RAGAS metrics scores\n",
        "- Cost: API costs for embedding, retrieval, and reranking operations\n",
        "- Latency: Response time measurements for each retriever\n",
        "\n",
        "The final report will include recommendations on which retriever performs best for this specific dataset and use case.\n",
        "\n",
        "[Read full article](https://thedataguy.pro/blog/evaluating-advanced-rag-retrievers/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "Open `Advanced_Retrieval_Evaluation` in `VS Code` and run `uv sync`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "### Required API Keys\n",
        "\n",
        "This notebook requires the following API keys to function properly:\n",
        "\n",
        "1. **OpenAI API Key** - For embeddings and LLM access\n",
        "2. **Cohere API Key** - For reranking capability\n",
        "3. **LangChain API Key** - For LangSmith tracing and evaluation\n",
        "4. **X AI API Key** - For synthetic data generation\n",
        "\n",
        "The API keys will be securely collected in the following cells using `getpass` to avoid exposing sensitive information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For LangSmith\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "dataset_name = \"Retrieval Evaulation - John Wick\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = dataset_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"XAI_API_KEY\"] = getpass.getpass(\"Enter your XAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXKHcZmKzDwT"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We can simply `wget` these from GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbSIGtzX3dS",
        "outputId": "0ce6514e-2479-4001-af24-824f987ce599"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv -O john_wick_1.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv -O john_wick_2.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv -O john_wick_3.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv -O john_wick_4.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today.\n",
        "\n",
        "- Self-Query: Wants as much metadata as we can provide\n",
        "- Time-weighted: Wants temporal data\n",
        "\n",
        "> NOTE: While we're creating a temporal relationship based on when these movies came out for illustrative purposes, it needs to be clear that the \"time-weighting\" in the Time-weighted Retriever is based on when the document was *accessed* last - not when it was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "documents = []\n",
        "\n",
        "for i in range(1, 5):\n",
        "  loader = CSVLoader(\n",
        "      file_path=f\"john_wick_{i}.csv\",\n",
        "      metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
        "  )\n",
        "\n",
        "  movie_docs = loader.load()\n",
        "  for doc in movie_docs:\n",
        "\n",
        "    # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
        "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
        "\n",
        "    # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
        "    doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
        "\n",
        "    # newer movies have a more recent \"last_accessed_at\"\n",
        "    doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4-i)\n",
        "\n",
        "  documents.extend(movie_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"JohnWick\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWick\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
        "\n",
        "def create_rag_chain(llm,retriever, template=RAG_TEMPLATE):\n",
        "    \"\"\"Create a RAG chain using the provided vectorstore and template.\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "    \n",
        "    return (\n",
        "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "        | {\"answer\": prompt | llm | StrOutputParser(), \"contexts\": itemgetter(\"context\")}\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Naive RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "naive_retrieval_chain = create_rag_chain(\n",
        "    chat_model, retriever=naive_retriever, template=RAG_TEMPLATE\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(documents)\n",
        "\n",
        "bm25_retrieval_chain = create_rag_chain(\n",
        "    chat_model, retriever=bm25_retriever, template=RAG_TEMPLATE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"answer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse - but the `I don't know` isn't great!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")\n",
        "\n",
        "contextual_compression_retrieval_chain = create_rag_chain(\n",
        "    chat_model, retriever=compression_retriever, template=RAG_TEMPLATE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"answer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")\n",
        "\n",
        "multi_query_retrieval_chain = create_rag_chain(\n",
        "    chat_model, retriever=multi_query_retriever, template=RAG_TEMPLATE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"answer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = documents\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = Qdrant(\n",
        "    collection_name=\"full_documents\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")\n",
        "\n",
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")\n",
        "parent_document_retriever.add_documents(parent_docs, ids=None)\n",
        "\n",
        "parent_document_retrieval_chain = create_rag_chain(\n",
        "    chat_model, retriever=parent_document_retriever, template=RAG_TEMPLATE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"answer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")\n",
        "\n",
        "ensemble_retrieval_chain = create_rag_chain(\n",
        "    chat_model, retriever=ensemble_retriever, template=RAG_TEMPLATE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"answer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Synthetic Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.run_config import RunConfig\n",
        "from ragas.testset import TestsetGenerator\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_xai import ChatXAI\n",
        "\n",
        "grok = ChatXAI(model=\"grok-3-latest\")\n",
        "# Create a RunConfig with rate limit settings\n",
        "my_run_config = RunConfig(\n",
        "    max_workers=8,      # Control concurrent requests (default is 16)\n",
        "    timeout=180,         # Maximum time to wait for a single operation (default is 180s)\n",
        "    max_retries=10,      # Maximum number of retry attempts (default is 10)\n",
        "    max_wait=120,         # Maximum wait time between retries (default is 60s)\n",
        "    exception_types=(Exception,)  # Types of exceptions to retry on\n",
        ")\n",
        "\n",
        "# Initialize the generator with the LLM and embedding model\n",
        "generator_llm = LangchainLLMWrapper(grok)\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\"))\n",
        "\n",
        "# Set the run config for both LLM and embeddings\n",
        "generator_llm.set_run_config(my_run_config)\n",
        "generator_embeddings.set_run_config(my_run_config)\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "\n",
        "# Use the run_config in your generate call\n",
        "dataset = generator.generate_with_langchain_docs(\n",
        "    documents=documents, \n",
        "    testset_size=10,\n",
        "    run_config=my_run_config\n",
        ")\n",
        "# Save the testset to a CSV file\n",
        "dataset.to_pandas().to_csv(\"testset.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload LangSmith DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from ragas.integrations.langsmith import upload_dataset\n",
        "\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "import typing as t\n",
        "\n",
        "from langchain.smith import RunEvalConfig\n",
        "\n",
        "from ragas.integrations.langchain import EvaluatorChain\n",
        "\n",
        "if t.TYPE_CHECKING:\n",
        "    from langsmith.schemas import Dataset as LangsmithDataset\n",
        "\n",
        "    from ragas.testset import Testset\n",
        "\n",
        "try:\n",
        "    from langsmith import Client\n",
        "    from langsmith.utils import LangSmithNotFoundError\n",
        "except ImportError:\n",
        "    raise ImportError(\n",
        "        \"Please install langsmith to use this feature. You can install it via pip install langsmith\"\n",
        "    )\n",
        "\n",
        "\n",
        "def upload_dataset(\n",
        "    dataset: pd.DataFrame, dataset_name: str, dataset_desc: str = \"\"\n",
        ") -> LangsmithDataset:\n",
        "    \"\"\"\n",
        "    Uploads a new dataset to LangSmith, converting it from a TestDataset object to a\n",
        "    pandas DataFrame before upload. If a dataset with the specified name already\n",
        "    exists, the function raises an error.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : TestDataset\n",
        "        The dataset to be uploaded.\n",
        "    dataset_name : str\n",
        "        The name for the new dataset in LangSmith.\n",
        "    dataset_desc : str, optional\n",
        "        A description for the new dataset. The default is an empty string.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    LangsmithDataset\n",
        "        The dataset object as stored in LangSmith after upload.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If a dataset with the specified name already exists in LangSmith.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The function attempts to read a dataset by the given name to check its existence.\n",
        "    If not found, it proceeds to upload the dataset after converting it to a pandas\n",
        "    DataFrame. This involves specifying input and output keys for the dataset being\n",
        "    uploaded.\n",
        "    \"\"\"\n",
        "    client = Client()\n",
        "    try:\n",
        "        # check if dataset exists\n",
        "        langsmith_dataset: LangsmithDataset = client.read_dataset(\n",
        "            dataset_name=dataset_name\n",
        "        )\n",
        "        raise ValueError(\n",
        "            f\"Dataset {dataset_name} already exists in langsmith. [{langsmith_dataset}]\"\n",
        "        )\n",
        "    except LangSmithNotFoundError:\n",
        "        # if not create a new one with the generated query examples\n",
        "        langsmith_dataset: LangsmithDataset = client.upload_dataframe(\n",
        "            df=dataset,\n",
        "            name=dataset_name,\n",
        "            input_keys=[\"question\"],\n",
        "            output_keys=[\"ground_truth\"],\n",
        "            #metadata_keys=[\"context\"],\n",
        "            description=dataset_desc,\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"Created a new dataset '{langsmith_dataset.name}'. Dataset is accessible at {langsmith_dataset.url}\"\n",
        "        )\n",
        "        return langsmith_dataset\n",
        "    \n",
        "# Load the test set from a CSV file\n",
        "\n",
        "df = pd.read_csv(\"testset.csv\")\n",
        "# Convert string representations of lists to actual Python lists\n",
        "df['reference_contexts'] = df['reference_contexts'].apply(ast.literal_eval)\n",
        "# set columns to question, context, ground_truth\n",
        "df = df.rename(columns={\n",
        "    'user_input': 'question',\n",
        "    'reference_contexts': 'context',\n",
        "    'reference': 'ground_truth'\n",
        "})\n",
        "\n",
        "upload_dataset(\n",
        "    dataset=df,\n",
        "    dataset_name=dataset_name,\n",
        "    dataset_desc=\"A test set of John Wick reviews\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.integrations.langsmith import evaluate\n",
        "from ragas.metrics import context_recall, faithfulness, context_precision, answer_relevancy\n",
        "\n",
        "# build the evaluation metrics\n",
        "metrics = [answer_relevancy, context_precision, faithfulness, context_recall]\n",
        "\n",
        "# Create a list of chains to evaluate\n",
        "chain_list = [\n",
        "    (\"Naive Retrieval\", naive_retrieval_chain),\n",
        "    (\"BM25 Retrieval\", bm25_retrieval_chain),\n",
        "    (\"Parent Document Retrieval\", parent_document_retrieval_chain),\n",
        "    (\"Contextual Compression Retrieval\", contextual_compression_retrieval_chain),\n",
        "    (\"Multi-Query Retrieval\", multi_query_retrieval_chain),\n",
        "    (\"Ensemble Retrieval\", ensemble_retrieval_chain),\n",
        "]\n",
        "\n",
        "\n",
        "# Run evaluation on each chain\n",
        "for chain_name, chain in chain_list:\n",
        "    print(f\"Evaluating {chain_name}...\")\n",
        "\n",
        "    evaluate(\n",
        "        dataset_name=dataset_name,\n",
        "        llm_or_chain_factory=chain,\n",
        "        experiment_name=f\"{chain_name}\",\n",
        "        metrics=metrics,\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
